[build-system]
requires = ["hatchling", "maturin>=1.7,<2.0"]
build-backend = "hatchling.build"

[project]
name = "pilot"
version = "0.1.0"
description = "Pilot project"
requires-python = ">=3.12"
dependencies = [
    "flwr[simulation]>=1.22.0",
    "ray>=2.0.0",
    "pandas>=2.1.0",
    "torch==2.8.0",
    "tqdm",
    "transformers",
    "wandb",
    "redis>=5.0.0",
    "exalsius-api-client @ git+https://github.com/exalsius/exalsius-api-spec.git#subdirectory=client-sdk",
    "vessim[sil]>=0.13.0",

    # Utilities
    "accelerate",
    "appdirs",
    "datasets",
    "fire",

    # Nanochat dependencies
    "filelock",
    "pyarrow",
    "requests",
    "tokenizers",
    "tiktoken>=0.11.0",  # tiktoken for efficient inference
]

[dependency-groups]
dev = [
    "maturin>=1.9.4",
    "pytest>=8.0.0",
    "black>=25.1.0",
    "isort>=6.0.1",
    "pre-commit>=4.1.0",
    "ruff>=0.9.7",
]

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["pilot", "nanochat"]

[tool.maturin]
module-name = "rustbpe"
bindings = "pyo3"
python-source = "."
manifest-path = "rustbpe/Cargo.toml"

[tool.flwr.app]
publisher = "exalsius"

[tool.flwr.app.components]
serverapp = "pilot.server_app:app"
clientapp = "pilot.client_app:app"

[tool.flwr.app.config]
redis_url = "redis://localhost:6379"
round_min_duration = 60

log_interval = 10  # Log client metrics every N batches
wandb_project = "pilot_flwr"
# wandb_entity = ""  # Optional wandb username / team name
# debug_port_server = 5681
# debug_port_client = 5682

# exls_cluster_id = "your-cluster-uuid"
forecast_api_url = "http://localhost:8700"
clients = {hyperstack = {exls_node_id = "uuid-1", provision_threshold = 100, deprovision_threshold = 200}, amd = {exls_node_id = "uuid-2", provision_threshold = 80, deprovision_threshold = 150}}


# ===== Nanochat Configuration (GPT Pretraining) =====
# Original nanochat hyperparameters from karpathy/nanochat
# Baseline: 4h on 8xH100 for 11.2B tokens (560M param model)
task_type = "nanochat"
model_type = "nanochat_d20"  # d20 = 20 layers, 560M params
dataset_name = "karpathy/fineweb-edu-100b-shuffle"
num_shards = 240  # Number of parquet files to use (each ~53k rows)
lr = 0.0006  # nanochat's original learning rate
weight_decay = 0.01
batch_size = 2  # Small batch for testing
max_length = 512  # Reduced for testing (original: 2048)
gradient_accumulation_steps = 1  # Set to 8 to simulate 8 GPUs on 1 GPU
# Model architecture (nanochat d20 defaults):
#   vocab_size = 65536
#   n_layer = 20 (depth)
#   n_embd = 1280 (hidden size)
#   n_head = 10 (attention heads)
#   n_kv_head = 10 (KV heads for MQA)


[tool.flwr.federations]
default = "local-simulation"

[tool.flwr.federations.local-simulation]
options.num-supernodes = 1

[tool.flwr.federations.local-simulation-gpu]
options.num-supernodes = 1
options.backend.client-resources.num-gpus = 1
options.backend.client-resources.num-cpus = 8

[tool.flwr.federations.local-deployment]
address = "127.0.0.1:9093"
insecure = true
